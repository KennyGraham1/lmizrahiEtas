# ETAS Parallel Simulation Workflow - Deep Technical Explanation

This document provides a comprehensive, code-level explanation of the `run_parallel_simulations.py` script and the underlying ETAS library.

---

## Table of Contents
1. [Overview](#overview)
2. [Phase 1: Preparation](#phase-1-preparation)
3. [Phase 2: Inversion (Learning)](#phase-2-inversion-learning)
4. [Phase 3: Simulation (Forecasting)](#phase-3-simulation-forecasting)
5. [Output Files](#output-files)
6. [Performance Optimizations](#performance-optimizations)

---

## Overview

The ETAS (Epidemic-Type Aftershock Sequence) model treats earthquake catalogs as a branching point process where:
- **Background events** (spontaneous, independent of other earthquakes) occur at a rate $\mu$.
- **Triggered events** (aftershocks) are generated by previous earthquakes according to the Omori-Utsu temporal decay law and a spatial power-law kernel.

The script performs a **retrospective forecasting experiment**:
1. For each date in the list, train the model using only data up to that date.
2. Generate probabilistic forecasts for the next 7 days.
3. Output can be compared to the actual observed earthquakes for validation.

---

## Phase 1: Preparation

### 1.1 Data Loading (`run_parallel_simulations.py`, lines 50-52)
```python
fn_catalog = "../input_data/nzcat.csv"
nzcat = pd.read_csv(fn_catalog, index_col=0, parse_dates=["time"])
nzcat.sort_values(by="time", inplace=True)
```
- Loads the New Zealand earthquake catalog (columns: `time`, `latitude`, `longitude`, `magnitude`, etc.)
- Sorts chronologically for proper temporal slicing.

### 1.2 Experiment Dates (`run_parallel_simulations.py`, lines 55-120)
```python
dates = [
    dt.datetime(2016, 11, 13, 12, 0, 0),
    dt.datetime(2016, 11, 14, 12, 0, 0),
    ...
]
```
Each date represents a **forecast origin time**. The model is trained on data *before* this time and forecasts *after* it.

### 1.3 Configuration (`run_parallel_simulations.py`, lines 122-146)
```python
inversion_config_base = {
    "fn_catalog": fn_catalog,
    "auxiliary_start": "1950-01-01 00:00:00",  # Auxiliary period for seismicity history
    "timewindow_start": "1960-01-01 00:00:00", # Start of training window
    # "timewindow_end" will be set dynamically to each date
    "theta_0": {...},  # Initial parameter guesses
    "mc": 4.1,         # Magnitude completeness
    ...
}
```

---

## Phase 2: Inversion (Learning)

### 2.1 The EM Algorithm (`inversion.py`, `ETASParameterCalculation.invert()`)

The inversion uses an **Expectation-Maximization (EM)** algorithm to find the Maximum Likelihood Estimate (MLE) of the ETAS parameters.

#### Convergence Loop (lines 1281-1319)
```python
while diff_to_before >= 0.001:
    # E-step: Calculate expected triggering probabilities
    self.pij, self.target_events, self.source_events, self.n_hat, self.i_hat = \
        self.expectation_step(theta_old, self.m_ref - self.delta_m / 2)
    
    # M-step: Optimize parameters given the current probabilities
    self.__theta = self.optimize_parameters(theta_old)
    
    # Check for convergence
    diff_to_before = calc_diff_to_before(theta_old, self.__theta)
    theta_old = self.__theta[:]
```

### 2.2 Expectation Step (E-Step)

The E-step computes the **triggering probabilities** $P_{ij}$: The probability that earthquake $j$ was triggered by earthquake $i$.

#### Triggering Kernel (`inversion.py`, `triggering_kernel()`)
For each source-target pair, the kernel computes:

$$
\lambda_{ij} = \kappa_i \cdot g(t) \cdot f(r)
$$

Where:
- $\kappa_i = k_0 \cdot e^{\alpha (m_i - m_c)}$ — Productivity (expected number of aftershocks)
- $g(t) = \frac{e^{-t/\tau}}{(t+c)^{1+\omega}}$ — Temporal decay (modified Omori-Utsu)
- $f(r) = \frac{1}{(r^2 + d \cdot e^{\gamma(m_i-m_c)})^{1+\rho}}$ — Spatial decay (isotropic power law)

**JIT Optimization** (`_triggering_kernel_core`, lines 334-367):
```python
@jit(nopython=True, cache=True, parallel=True)
def _triggering_kernel_core(...):
    for i in prange(n):  # Parallel loop across all sources
        aftershock_num = k0 * np.exp(a * (m[i] - mc))
        time_decay = np.exp(-t / tau) / np.power(t + c, 1.0 + omega)
        space_decay = 1.0 / np.power(s + d_scaled, 1.0 + rho)
        result[i] = aftershock_num * time_decay * space_decay
    return result
```

### 2.3 Maximization Step (M-Step)

The M-step maximizes the **log-likelihood** with respect to the parameters.

#### Log-Likelihood Function (`inversion.py`, `neg_log_likelihood()`)

The negative log-likelihood has two terms:

1. **Aftershock Term**: How well does the expected number of aftershocks match the observed count?
   ```python
   aftershock_term = ll_aftershock_term(source_events["l_hat"], source_events["G"]).sum()
   ```

2. **Distribution Term**: How well does the space-time PDF match the observed events?
   ```python
   distribution_term = sum(P_ij * likelihood_term_ij)
   ```

**JIT Optimization** (`_neg_log_likelihood_core`, lines 522-568):
The distribution term calculation is parallelized across millions of source-target pairs.

### 2.4 Output of Inversion

For each model (date), the following files are saved:
- `parameters_*.json`: Final ETAS parameter estimates.
- `pij_*.csv`: The full probability matrix (can be GBs for large catalogs).
- `sources_*.csv`: Metadata about source events.

---

## Phase 3: Simulation (Forecasting)

### 3.1 Simulation Overview (`simulation.py`, `generate_catalog()`)

The simulation generates synthetic earthquake catalogs using a **branching process**:

```python
while True:
    sources = catalog.query("generation == @generation and n_aftershocks > 0")
    if len(sources) == 0:
        break  # No more aftershocks → stop
    aftershocks = generate_aftershocks(sources, ...)
    catalog = pd.concat([catalog, aftershocks])
    generation += 1
```

### 3.2 Background Event Generation

Background events are generated from a Poisson process with rate $\mu$:
- **Timing**: Uniformly distributed within the forecast window.
- **Location**: Sampled from the spatial distribution of source events (kernel density estimation).
- **Magnitude**: Sampled from a Gutenberg-Richter distribution.

### 3.3 Aftershock Generation (`simulation.py`, `generate_aftershocks()`)

For each parent event, aftershocks are generated:

#### Step 3.3.1: Number of Aftershocks
```python
expected_n = k0 * exp(alpha * (m_parent - mc))
n_aftershocks = Poisson(expected_n)
```

#### Step 3.3.2: Time Delays
Uses the inverse CDF method to sample from the Omori-Utsu distribution.

**JIT Optimization** (`_inv_time_cdf_approx_core`, uses `approx_times=True`):
```python
@jit(nopython=True, cache=True, parallel=True)
def _inv_time_cdf_approx_core(p, c, tau, omega):
    for i in prange(n):
        if p[i] < threshold:
            result[i] = c * (np.power(1 - p[i], -1.0 / omega) - 1)  # Power-law approx
        else:
            result[i] = -tau * np.log(1 - p[i]) - c  # Exponential approx
    return result
```

#### Step 3.3.3: Spatial Location
Uses the inverse CDF of the isotropic power-law spatial kernel.

**JIT Optimization** (`_simulate_aftershock_radius_core`):
```python
@jit(nopython=True, cache=True, parallel=True)
def _simulate_aftershock_radius_core(d, gamma, rho, mi, mc, random_values):
    for i in prange(n):
        d_g = d * np.exp(gamma * (mi[i] - mc))
        result[i] = np.sqrt(np.power(1 - random_values[i], -1 / rho) * d_g - d_g)
    return result
```

#### Step 3.3.4: Magnitude
Sampled from a Gutenberg-Richter distribution (exponential distribution of magnitudes):
```python
magnitude = mc - log(1 - U) / beta  # where U ~ Uniform(0, 1)
```

### 3.4 Cascade Propagation

The aftershocks themselves become parents for the next generation. The loop continues until no more aftershocks are generated within the forecast window. This naturally captures the cascade dynamics of earthquake sequences.

---

## Output Files

| Directory | File Pattern | Description |
|-----------|--------------|-------------|
| `output_nz/` | `parameters_nz_Kaikoura_X.json` | Learned ETAS parameters for model X |
| `output_nz/` | `pij_nz_Kaikoura_X.csv` | Triggering probability matrix (large!) |
| `output_nz/` | `sources_nz_Kaikoura_X.csv` | Source event metadata |
| `simulations_nz/` | `sim_nz_Kaikoura_X_Y.csv` | Synthetic catalog (X=model, Y=chunk) |

### Simulation File Naming Convention

The simulation output files follow this naming pattern:

```
sim_nz_Kaikoura_2_5.csv
│   │  │        │ │
│   │  │        │ └─ file_no (Y) = chunk index (0-9 when n_files=10)
│   │  │        └─── date_index (X) = index into dates array
│   │  └──────────── sequence = earthquake sequence name
│   └─────────────── region = study region code
└─────────────────── prefix = simulation output marker
```

| Component | Example | Meaning |
|-----------|---------|---------|
| `sim_` | - | Identifies this as simulation output |
| `nz_` | - | New Zealand region |
| `Kaikoura` | - | Kaikoura earthquake sequence (vs. Canterbury) |
| `2` (X) | `dates[2]` | 3rd forecast date: `2016-11-15 12:00:00` |
| `5` (Y) | Chunk 5 | Simulation batch 5 of 10 (contains sims 501-600) |

**How simulations are distributed across files:**

With default settings (`n_files=10`, `n_simulations_overall=10000`):
- Each model/date gets split into 10 file chunks
- Each chunk contains `10000 / 10 = 1000` simulations
- File `_X_Y.csv` contains simulations `(Y * 1000)` through `((Y+1) * 1000 - 1)`

**Example:** For Kaikoura model index 4:
- `sim_nz_Kaikoura_4_0.csv` → simulations 0-999
- `sim_nz_Kaikoura_4_1.csv` → simulations 1000-1999
- ...
- `sim_nz_Kaikoura_4_9.csv` → simulations 9000-9999

---

## Performance Optimizations

The codebase has been optimized with Numba JIT compilation for critical loops:

| Function | Optimization | Speedup |
|----------|--------------|---------|
| `_triggering_kernel_core` | Parallel JIT | 10-100x |
| `_neg_log_likelihood_core` | Parallel JIT | 5-10x |
| `_simulate_aftershock_radius_core` | Parallel JIT | 5-10x |
| `_inv_time_cdf_approx_core` | Parallel JIT | 5-10x |

**Memory Optimization**: JIT functions avoid creating intermediate Pandas Series, reducing memory footprint by ~50% for large catalogs.

**Parallelization**: Inversions run with ~12 workers, simulations with ~40 workers, dynamically adjusted based on available RAM.

---

## Parameter Definitions

| Parameter | Symbol | Description |
|-----------|--------|-------------|
| `log10_mu` | $\log_{10}(\mu)$ | Background rate (events/day/km²) |
| `log10_k0` | $\log_{10}(k_0)$ | Aftershock productivity constant |
| `a` | $\alpha$ | Magnitude efficiency for aftershock generation |
| `log10_c` | $\log_{10}(c)$ | Omori-Utsu time offset (days) |
| `omega` | $\omega$ | Modified Omori-Utsu exponent ($\omega = p - 1$) |
| `log10_tau` | $\log_{10}(\tau)$ | Exponential taper time constant |
| `log10_d` | $\log_{10}(d)$ | Spatial kernel scale (km²) |
| `gamma` | $\gamma$ | Magnitude scaling of spatial kernel |
| `rho` | $\rho$ | Spatial kernel decay exponent |

---

## How to Use the Output

### Forecast Validation
Once simulations are complete, you can compare your forecasts against **observed data** using standard CSEP tests:

```python
import pandas as pd
import numpy as np

# Load simulations for a specific model
sim_files = [f"simulations_nz/sim_nz_Kaikoura_4_{i}.csv" for i in range(10)]
simulations = pd.concat([pd.read_csv(f) for f in sim_files])

# Load actual observed catalog
observed = pd.read_csv("input_data/nzcat.csv", parse_dates=["time"])
forecast_start = pd.Timestamp("2016-11-16 12:00:00")
forecast_end = forecast_start + pd.Timedelta(days=7)
observed = observed[(observed["time"] > forecast_start) & (observed["time"] <= forecast_end)]

# N-Test: Compare number of events
simulated_counts = simulations.groupby("simulation_id").size()
observed_count = len(observed)
quantile = (simulated_counts < observed_count).mean()
print(f"N-Test: Observed {observed_count} events, quantile = {quantile:.3f}")
# If quantile is between 0.025 and 0.975, the forecast is consistent.
```

### Parameter Evolution
Plot how parameters change as more data becomes available:
```python
import json
import matplotlib.pyplot as plt

params = []
for i in range(20):
    with open(f"output_nz/parameters_nz_Kaikoura_{i}.json") as f:
        params.append(json.load(f)["final_parameters"])

plt.plot([p["log10_k0"] for p in params], label="log10(k0)")
plt.xlabel("Days after mainshock")
plt.ylabel("Parameter value")
plt.legend()
plt.show()
```

---

## Known Limitations

| Limitation | Description |
|------------|-------------|
| **Isotropic Spatial Kernel** | The current implementation assumes circular symmetry. Real faults produce elongated aftershock zones along the rupture. |
| **Fixed Magnitude Distribution** | Uses a Gutenberg-Richter law with constant $b$-value. In reality, $b$ can vary spatially and temporally. |
| **No Depth Modeling** | Events are treated as 2D (lat/lon only). Depth is not simulated. |
| **Stationary Background Rate** | The background rate $\mu$ is constant over time. In reality, it may vary due to tectonic loading. |
| **No Incompleteness Modeling** | Assumes the catalog is complete above $M_c$. Early aftershocks may be missed due to coda wave interference. |

---

## References

This implementation is based on the following publications:

1. **Mizrahi, L., Nandan, S., & Wiemer, S. (2021).** *The effect of declustering on the size distribution of mainshocks.* Seismological Research Letters.

2. **Ogata, Y. (1988).** *Statistical models for earthquake occurrences and residual analysis for point processes.* Journal of the American Statistical Association, 83(401), 9-27.

3. **Zhuang, J., Ogata, Y., & Vere-Jones, D. (2002).** *Stochastic declustering of space-time earthquake occurrences.* Journal of the American Statistical Association, 97(458), 369-380.

4. **Helmstetter, A., & Sornette, D. (2002).** *Subcritical and supercritical regimes in epidemic models of earthquake aftershocks.* Journal of Geophysical Research, 107(B10).

---

## Quick Command Reference

```bash
# Run the full workflow
cd examples && python run_parallel_simulations.py

# Check inversion progress
ls output_nz/parameters_*.json | wc -l

# Check simulation progress  
ls simulations_nz/*.csv | wc -l

# View learned parameters
cat output_nz/parameters_nz_Kaikoura_0.json | python -m json.tool
```

---

## Troubleshooting

| Problem | Cause | Solution |
|---------|-------|----------|
| `MemoryError` during inversion | `pij` matrix is too large | Reduce `coppersmith_multiplier` or increase `mc` |
| Very slow inversions | Large catalog or many iterations | Enable Numba JIT (install `numba`) |
| `RuntimeWarning: invalid value in log` | Numerical instability during optimization | Already fixed in latest code; update `inversion.py` |
| Simulations produce too few events | Low productivity ($k_0$) or high $M_c$ | Check learned parameters; lower $M_c$ if catalog supports it |
| Script hangs with no output | Joblib parallel deadlock | Reduce `N_JOBS_INVERSION` or run serially for debugging |

---

## Customization Guide

### Using a Different Region
1. Create a new polygon file (NumPy array of [lat, lon] vertices):
   ```python
   import numpy as np
   my_polygon = np.array([[-35, 170], [-35, 175], [-40, 175], [-40, 170], [-35, 170]])
   np.save("input_data/my_region.npy", my_polygon)
   ```
2. Update `inversion_config_base["shape_coords"]` to point to your file.

### Using a Different Catalog
1. Prepare a CSV with columns: `time`, `latitude`, `longitude`, `magnitude`
2. Update `fn_catalog` in the script.
3. Adjust `mc` (magnitude completeness) appropriately.

### Changing Forecast Duration
Edit line ~206 in `run_parallel_simulations.py`:
```python
forecast_period = 7.0  # Change to 1.0 for 1-day forecasts, 30.0 for monthly, etc.
```

### Changing Number of Simulations
Edit line ~205:
```python
n_simulations_overall = 10000  # Increase for better statistics, decrease for speed
```

### Fixing Specific Parameters
You can hold certain parameters constant during optimization:
```python
inversion_config_base["fixed_parameters"] = {
    "alpha": "beta",       # Constrain alpha = beta (Mark et al. assumption)
    "omega": -0.1,         # Fix omega to a specific value
    "rho": 0.8,            # Fix spatial decay exponent
}
```

### Custom Initial Parameter Guesses
Provide starting values for the optimizer:
```python
inversion_config_base["theta_0"] = {
    "log10_mu": -7.5,
    "log10_iota": None,        # Set to None if not using induced seismicity
    "log10_k0": -1.0,
    "a": 1.5,
    "log10_c": -3.0,
    "omega": -0.1,
    "log10_tau": 4.0,
    "log10_d": 1.5,
    "gamma": 0.3,
    "rho": 0.8
}
```

### Adjusting Parallelism
Control how many parallel workers are used:
```python
# For memory-constrained systems (edit lines 30-34):
N_JOBS_INVERSION = 4   # Fewer workers = less RAM usage
N_JOBS_SIMULATION = 8

# For high-memory systems:
N_JOBS_INVERSION = 16
N_JOBS_SIMULATION = 64
```

### Running a Single Inversion (Debug Mode)
For testing or debugging, run just one model without parallelism:
```python
from etas.inversion import ETASParameterCalculation
import json

config = {
    "fn_catalog": "../input_data/nzcat.csv",
    "timewindow_start": "1960-01-01",
    "timewindow_end": "2016-11-14 12:00:00",
    "auxiliary_start": "1950-01-01",
    "mc": 4.1,
    "m_ref": 4.1,
    "delta_m": 0.1,
    "shape_coords": "../input_data/nz_polygon.npy",
    "coppersmith_multiplier": 100
}

calc = ETASParameterCalculation(config)
calc.prepare()
theta = calc.invert()
print(json.dumps(calc.theta, indent=2))
```

### Running a Single Simulation (Debug Mode)
Test simulation without the full pipeline:
```python
from etas.inversion import ETASParameterCalculation
from etas.simulation import ETASSimulation
import json

# Load pre-computed parameters
with open("output_nz/parameters_nz_Kaikoura_0.json") as f:
    params = json.load(f)

params["fn_catalog"] = "../input_data/nzcat.csv"
params["shape_coords"] = "../input_data/nz_polygon.npy"

calc = ETASParameterCalculation.load_calculation(params)
sim = ETASSimulation(calc, approx_times=True)
sim.prepare()

# Run 100 simulations for 7 days
for chunk in sim.simulate(forecast_duration=7.0, n_simulations=100):
    print(f"Generated {len(chunk)} events")
```

### Changing the Convergence Threshold
Make inversions faster (less accurate) or slower (more accurate):
```python
# In inversion.py, ETASParameterCalculation.invert(), line ~1281:
while diff_to_before >= 0.001:  # Default: 0.001
# Change to 0.01 for faster convergence, 0.0001 for more precision
```

### Using Free Productivity Mode
Let each source event have its own productivity instead of a global $k_0$:
```python
inversion_config_base["free_productivity"] = True
```
This can improve fits but increases computational cost.

### Using Free Background Mode
Allow spatially varying background rate instead of uniform $\mu$:
```python
inversion_config_base["free_background"] = True
```

### Setting Maximum Magnitude
Cap the maximum magnitude that can be simulated:
```python
simulation = ETASSimulation(calc, m_max=8.0, approx_times=True)
```

### Induced Seismicity Mode
If your catalog contains induced events (e.g., from injection):
```python
inversion_config_base["induced_info"] = [
    {"event_id": 12345, "injection_rate": 100},  # m³/day
    ...
]
```

### Disabling Pij Storage (Save Disk Space)
The `pij_*.csv` files are huge. To skip saving them:
```python
# In run_parallel_simulations.py, run_inversion function:
calculation.store_results(config['data_path'], store_pij=False)  # Change True → False
```

### Custom Magnitude Distribution
Use zone-based magnitude-frequency distributions:
```python
# Define zones with different b-values
mfd_zones = {
    "zone_A": {"beta": 2.3, "mc": 3.5},
    "zone_B": {"beta": 2.0, "mc": 4.0},
}

def zones_from_latlon(lat, lon):
    # Return zone name based on coordinates
    return np.where(lat > -40, "zone_A", "zone_B")

# Pass to generate_aftershocks:
generate_aftershocks(..., mfd_zones=mfd_zones, zones_from_latlon=zones_from_latlon)
```

### Changing the Auxiliary Period
The auxiliary period provides historical context without contributing to likelihood:
```python
inversion_config_base["auxiliary_start"] = "1900-01-01"  # Go further back
inversion_config_base["timewindow_start"] = "1980-01-01"  # But train from here
```

### Output File Naming Convention
Change the model ID format:
```python
# In run_inversion function:
model_id = f"my_experiment_{date.strftime('%Y%m%d')}"  # e.g., "my_experiment_20161114"
```

---

## Workflow Diagram

```
┌─────────────────────────────────────────────────────────────────┐
│                    EARTHQUAKE CATALOG                          │
│              (nzcat.csv: time, lat, lon, mag)                   │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                    FOR EACH DATE t:                             │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ INVERSION (EM Algorithm)                                  │  │
│  │  1. E-step: Calculate P(event j triggered by event i)     │  │
│  │  2. M-step: Optimize θ = (μ, k₀, α, c, ω, τ, d, γ, ρ)     │  │
│  │  3. Repeat until convergence                              │  │
│  └───────────────────────────────────────────────────────────┘  │
│                              │                                   │
│                              ▼                                   │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ SIMULATION (Branching Process)                            │  │
│  │  1. Generate background events (Poisson, rate μ)          │  │
│  │  2. For each parent:                                      │  │
│  │     - Sample N ~ Poisson(κ)                               │  │
│  │     - Sample time delays (Omori-Utsu)                     │  │
│  │     - Sample locations (power-law)                        │  │
│  │     - Sample magnitudes (Gutenberg-Richter)               │  │
│  │  3. Repeat for all generations until none left            │  │
│  └───────────────────────────────────────────────────────────┘  │
│                              │                                   │
│                              ▼                                   │
│              OUTPUT: 10,000 synthetic catalogs                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## FAQ

### General Questions

**Q: How long does a full run take?**
A: Depends on catalog size and number of dates. For NZ with ~55 dates and 10k simulations each, expect 4-8 hours on an 80-core machine. Inversions take ~10-20 min each, simulations ~2-5 min each.

**Q: Why are the `pij_*.csv` files so large (6GB each)?**
A: They store the full N×M probability matrix where N = source events and M = target events. For a catalog with 20k source events and 17k target events, that's 340 million rows! You can delete these after the run if you only need the parameters, or set `store_pij=False`.

**Q: Can I run just simulations without re-running inversions?**
A: Yes! The script automatically skips inversions if `parameters_*.json` files exist. Just make sure the parameter files are in `output_nz/`.

**Q: What is the branching ratio?**
A: It's the expected number of direct aftershocks per earthquake (averaged over magnitude distribution). If > 1, the cascade is "supercritical" and would grow forever (unrealistic). Typical values are 0.7-0.95.

**Q: How do I add more dates to the experiment?**
A: Simply append more `dt.datetime(...)` entries to the `dates` list in the script.

---

### Parameter Questions

**Q: What does a negative omega mean?**
A: Omega is related to the Omori-Utsu p-value by omega = p - 1. A negative omega (e.g., -0.1) means p = 0.9. Most earthquake sequences have p in the range 1.0 to 1.2, so omega typically ranges from -0.1 to 0.2.

**Q: Why is log10_tau so large (e.g., 4.0)?**
A: Log10_tau = 4.0 means tau = 10,000 days (~27 years). This is the exponential taper time constant. A large value means the temporal decay is mostly power-law with minimal exponential cutoff.

**Q: What's the difference between `a` and `alpha`?**
A: In this codebase, `a` in the parameter array corresponds to alpha in the literature. It controls how larger earthquakes produce more aftershocks.

**Q: Why is log10_mu negative (e.g., -8)?**
A: Because mu is very small! Log10_mu = -8 means mu = 10^-8 events per day per km². For a 1 million km² region over 10,000 days, that's about 100 background events.

**Q: What is gamma?**
A: Gamma controls how larger earthquakes have larger aftershock zones. The characteristic distance scales as d * 10^(gamma*(m - mc)). Typical values are 0.3-0.5.

**Q: What is rho?**
A: Rho is the spatial decay exponent. Larger values mean faster decay with distance. Values typically range from 0.5 to 1.5.

---

### Performance Questions

**Q: How can I make inversions faster?**
A: Several options:
1. Install Numba for JIT compilation (automatic 5-10x speedup)
2. Increase convergence threshold: change 0.001 to 0.01 in invert()
3. Reduce catalog size by increasing mc (fewer events)
4. Reduce coppersmith_multiplier (smaller interaction radius)

**Q: How can I reduce memory usage?**
A: 
1. Reduce N_JOBS_INVERSION (fewer parallel workers)
2. Set store_pij=False to skip saving the huge probability matrices
3. Increase mc to reduce catalog size

**Q: Why is Numba not being used?**
A: Check that numba is installed: `pip install numba`. Then verify with `python -c "from numba import jit; print('OK')"`.

**Q: Can I use GPU acceleration?**
A: Not currently. The codebase uses CPU-based Numba JIT. GPU support would require significant code changes.

---

### Accuracy Questions

**Q: Does `approx_times=True` affect accuracy?**
A: Minimally. The approximation uses a piecewise function (power-law + exponential) that matches the exact distribution very closely. For forecasting purposes, the difference is negligible.

**Q: How accurate is the isotropic spatial kernel?**
A: It's a simplification. Real aftershock zones are elongated along the fault strike. For small earthquakes (M < 5), isotropic is reasonable. For large earthquakes, an anisotropic kernel would be more accurate but is not yet implemented.

**Q: How many simulations do I need?**
A: For robust statistics:
- N-test: 1,000 simulations is usually sufficient
- Spatial likelihood: 5,000-10,000 simulations recommended
- Rare event probabilities (e.g., P(M>=7)): 10,000+ simulations needed

---

### Output Questions

**Q: What's in the simulation CSV files?**
A: Each row is a simulated earthquake with columns: time, latitude, longitude, magnitude, generation (0=background, 1+=aftershock), parent (ID of parent event), simulation_id.

**Q: How do I combine multiple simulation chunks?**
```python
import pandas as pd
chunks = [pd.read_csv(f"sim_nz_Kaikoura_0_{i}.csv") for i in range(10)]
all_sims = pd.concat(chunks, ignore_index=True)
```

**Q: What's in the parameters JSON?**
A: Key fields: final_parameters (learned ETAS params), initial_values, n_iterations, n_hat (background count), beta (b-value), timewindow_end (forecast origin).

**Q: Can I load parameters and run my own analysis?**
```python
import json
with open("output_nz/parameters_nz_Kaikoura_10.json") as f:
    params = json.load(f)
print(f"Background rate: 10^{params['final_parameters']['log10_mu']:.2f}")
print(f"Productivity: 10^{params['final_parameters']['log10_k0']:.2f}")
```

---

### Troubleshooting

**Q: Inversion doesn't converge (runs forever)?**
A: Possible causes: Poor initial values (try different theta_0), oscillating parameters (increase convergence threshold), or numerical issues.

**Q: Simulations produce zero events?**
A: Check: Is the background rate (mu) too low? Is the forecast window too short? Are there events in the catalog before the forecast start?

**Q: Getting NaN in parameters?**
A: Usually caused by empty source/target event sets, division by zero in likelihood, or log of negative number.

**Q: Script crashes with "Killed" message?**
A: Out of memory (OOM). Reduce N_JOBS_INVERSION, increase swap space, or process fewer dates at a time.

---

## Glossary

| Term | Definition |
|------|------------|
| **ETAS** | Epidemic-Type Aftershock Sequence - a statistical model for earthquake clustering |
| **Background event** | Earthquake not triggered by another earthquake (spontaneous) |
| **Triggered event** | Earthquake (aftershock) caused by a previous earthquake |
| **Branching ratio** | Expected number of direct aftershocks per event; must be < 1 for stability |
| **Omori-Utsu law** | Empirical law describing temporal decay of aftershock rates |
| **Gutenberg-Richter law** | Empirical law relating earthquake frequency to magnitude |
| **Completeness magnitude (Mc)** | Minimum magnitude above which all earthquakes are detected |
| **Productivity** | Number of aftershocks generated by an earthquake (κ or k0) |
| **E-step** | Expectation step in EM algorithm - compute triggering probabilities |
| **M-step** | Maximization step in EM algorithm - optimize parameters |
| **Pij matrix** | Matrix of probabilities P(event j triggered by event i) |
| **Coppersmith** | Scaling relation for rupture dimensions based on magnitude |
| **N-test** | Forecast evaluation: compare predicted vs observed event counts |
| **L-test** | Forecast evaluation: spatial likelihood of observed events |

---

## Related Tools and Packages

| Package | Description | Link |
|---------|-------------|------|
| **pyCSEP** | Python tools for CSEP forecast evaluation | [github.com/SCECcode/pycsep](https://github.com/SCECcode/pycsep) |
| **floatCSEP** | Floating window CSEP experiments | [github.com/floatCSEP](https://github.com/floatCSEP) |
| **SeismoStats** | Statistical tools for seismology | Included in this repo |
| **OpenQuake** | Seismic hazard and risk assessment | [github.com/gem/oq-engine](https://github.com/gem/oq-engine) |
| **ZMAP** | Seismicity analysis in MATLAB | [github.com/CelsoReyes/zmap7](https://github.com/CelsoReyes/zmap7) |

---

## How to Cite

If you use this code in your research, please cite:

```bibtex
@article{mizrahi2021etas,
  title={The effect of declustering on the size distribution of mainshocks},
  author={Mizrahi, Leila and Nandan, Shyam and Wiemer, Stefan},
  journal={Seismological Research Letters},
  year={2021},
  publisher={Seismological Society of America}
}
```

---

## Example Use Cases

### 1. Operational Earthquake Forecasting (OEF)
Real-time short-term forecasting during earthquake sequences:
- Update parameters daily as new data arrives
- Generate 1-day or 7-day forecasts
- Communicate probabilistic hazard to civil protection

### 2. Retrospective Forecast Validation
Test how well the model would have performed historically:
- Run multiple forecast origins during a known sequence
- Compare simulations against actual observations
- Calculate skill scores (N-test, L-test, information gain)

### 3. Declustering
Separate background events from triggered events:
- Use learned Pij probabilities
- Events with high P(background) are likely tectonic
- Events with high P(triggered) are likely aftershocks

### 4. Hazard Analysis
Estimate probabilities of large earthquakes:
- Run many simulations
- Count how often M >= 6 or M >= 7 events occur
- Report as exceedance probability

### 5. Scenario Testing
"What if" analysis for emergency planning:
- Inject a hypothetical large earthquake
- Simulate the expected aftershock sequence
- Estimate resource needs for response

---

## Version History

| Version | Date | Changes |
|---------|------|---------|
| 1.0 | 2024 | Original implementation by Mizrahi et al. |
| 1.1 | 2026-01 | Added Numba JIT optimization for inversion and simulation |
| 1.1.1 | 2026-01 | Fixed RuntimeWarning in log likelihood, enabled approx_times by default |

---

*Document generated for lmizrahiEtas codebase v1.1*

